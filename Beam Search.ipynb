{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/MaximumEntropy/Seq2Seq-PyTorch/blob/master/beam_search.py\n",
    "\n",
    "\"\"\"Beam search implementation in PyTorch.\"\"\"\n",
    "#\n",
    "#\n",
    "#         hyp1#-hyp1---hyp1 -hyp1\n",
    "#                 \\             /\n",
    "#         hyp2 \\-hyp2 /-hyp2#hyp2\n",
    "#                               /      \\\n",
    "#         hyp3#-hyp3---hyp3 -hyp3\n",
    "#         ========================\n",
    "#\n",
    "# Takes care of beams, back pointers, and scores.\n",
    "\n",
    "# Code borrowed from PyTorch OpenNMT example\n",
    "# https://github.com/pytorch/examples/blob/master/OpenNMT/onmt/Beam.py\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"Ordered beam of candidate outputs.\"\"\"\n",
    "\n",
    "    def __init__(self, size, vocab, cuda=False):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self.size = size\n",
    "        self.done = False\n",
    "        self.pad = vocab['<pad>']\n",
    "        self.bos = vocab['$start']\n",
    "        self.eos = vocab['$end']\n",
    "        self.tt = torch.cuda if cuda else torch\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [self.tt.LongTensor(size).fill_(self.pad)]\n",
    "        self.nextYs[0][0] = self.bos\n",
    "\n",
    "    # Get the outputs for the current timestep.\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get state of beam.\"\"\"\n",
    "        return self.nextYs[-1]\n",
    "\n",
    "    # Get the backpointers for the current timestep.\n",
    "    def get_current_origin(self):\n",
    "        \"\"\"Get the backpointer to the beam at this step.\"\"\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    #  Given prob over words for every last beam `wordLk`: Compute and update the beam search.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #\n",
    "    #     * `wordLk`- probs of advancing from the last step (K x words)\n",
    "    #\n",
    "    # Returns: True if beam search is complete.\n",
    "\n",
    "    def advance(self, workd_lk):\n",
    "        \"\"\"Advance the beam.\"\"\"\n",
    "        num_words = workd_lk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n",
    "        else:\n",
    "            beam_lk = workd_lk[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "\n",
    "        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0, True, True)\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prev_k = bestScoresId / num_words\n",
    "        self.prevKs.append(prev_k)\n",
    "        self.nextYs.append(bestScoresId - prev_k * num_words)\n",
    "\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.nextYs[-1][0] == self.eos:\n",
    "            self.done = True\n",
    "\n",
    "        return self.done\n",
    "\n",
    "    def sort_best(self):\n",
    "        \"\"\"Sort the beam.\"\"\"\n",
    "        return torch.sort(self.scores, 0, True)\n",
    "\n",
    "    # Get the score of the best in the beam.\n",
    "    def get_best(self):\n",
    "        \"\"\"Get the most likely candidate.\"\"\"\n",
    "        scores, ids = self.sort_best()\n",
    "        return scores[1], ids[1]\n",
    "\n",
    "    # Walk back to construct the full hypothesis.\n",
    "    #\n",
    "    # Parameters.\n",
    "    #\n",
    "    #     * `k` - the position in the beam to construct.\n",
    "    #\n",
    "    # Returns.\n",
    "    #\n",
    "    #     1. The hypothesis\n",
    "    #     2. The attention at each time step.\n",
    "    def get_hyp(self, k):\n",
    "        \"\"\"Get hypotheses.\"\"\"\n",
    "        hyp = []\n",
    "        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n",
    "        for j in range(len(self.prevKs) - 1, -1, -1):\n",
    "            hyp.append(self.nextYs[j + 1][k])\n",
    "            k = self.prevKs[j][k]\n",
    "\n",
    "        return hyp[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_predict_sample = torch.load('batch_predict_sample.trch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open('vocab_stoi_sample.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itos = pickle.load(open('vocab_itos_sample.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beam_size = 22\n",
    "beam = Beam(beam_size, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-7.8707 -1.3969 -4.5329  ...  -8.2039 -8.7784 -7.9060\n",
       "-7.8707 -1.3969 -4.5329  ...  -8.2039 -8.7784 -7.9060\n",
       "-7.8707 -1.3969 -4.5329  ...  -8.2039 -8.7784 -7.9060\n",
       "          ...             â‹±             ...          \n",
       "-7.8707 -1.3969 -4.5329  ...  -8.2039 -8.7784 -7.9060\n",
       "-7.8707 -1.3969 -4.5329  ...  -8.2039 -8.7784 -7.9060\n",
       "-7.8707 -1.3969 -4.5329  ...  -8.2039 -8.7784 -7.9060\n",
       "[torch.cuda.FloatTensor of size 81x1080 (GPU 0)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_predict_sample[:, 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(22):\n",
    "    beam.advance(torch.exp(batch_predict_sample[0, :, :]).data)\n",
    "# beam.sort_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam.get_hyp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array([beam.get_hyp(i) for i in range(beam_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> $start <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> $start <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(itos[i] for i in results.T[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-81f3901223b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_predict_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "torch.exp(batch_predict_sample)[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
